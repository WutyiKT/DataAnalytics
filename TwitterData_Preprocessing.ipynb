{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f13ec115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc22cad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "\n",
       "              tweetid  \n",
       "0  792927353886371840  \n",
       "1  793124211518832641  \n",
       "2  793124402388832256  \n",
       "3  793124635873275904  \n",
       "4  793125156185137153  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitter_sentiment_data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "145c5a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43943"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583a7e3",
   "metadata": {},
   "source": [
    "# Remove duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be99ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['message'], inplace=True)\n",
    "#df['original_message'] = df['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3842fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Unamshow awache kujinga na iko global warming ...</td>\n",
       "      <td>793125429418815489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @CCIRiviera: Presidential Candidate #Donald...</td>\n",
       "      <td>793126558688878592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @AmericanIndian8: Leonardo DiCaprio's clima...</td>\n",
       "      <td>793127097854197761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>#BeforeTheFlood Watch #BeforeTheFlood right he...</td>\n",
       "      <td>793127346106753028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @DrDeJarnett: It's vital that the public he...</td>\n",
       "      <td>793127915269480448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            message  \\\n",
       "0          -1  @tiniebeany climate change is an interesting h...   \n",
       "1           1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2           1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3           1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4           2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "5           0  Unamshow awache kujinga na iko global warming ...   \n",
       "7           2  RT @CCIRiviera: Presidential Candidate #Donald...   \n",
       "8           0  RT @AmericanIndian8: Leonardo DiCaprio's clima...   \n",
       "9           1  #BeforeTheFlood Watch #BeforeTheFlood right he...   \n",
       "10          1  RT @DrDeJarnett: It's vital that the public he...   \n",
       "\n",
       "               tweetid  \n",
       "0   792927353886371840  \n",
       "1   793124211518832641  \n",
       "2   793124402388832256  \n",
       "3   793124635873275904  \n",
       "4   793125156185137153  \n",
       "5   793125429418815489  \n",
       "7   793126558688878592  \n",
       "8   793127097854197761  \n",
       "9   793127346106753028  \n",
       "10  793127915269480448  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b97e72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41033"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7545f31",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066dd2f",
   "metadata": {},
   "source": [
    "# Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3be68c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before was:\n",
      " RT @NatGeoChannel: Watch #BeforeTheFlood right here, as @LeoDiCaprio travels the world to tackle climate change https://t.co/LkDehj3tNn httÃ¢â‚¬Â¦ \n",
      "Now it is:\n",
      " rt @natgeochannel: watch #beforetheflood right here, as @leodicaprio travels the world to tackle climate change https://t.co/lkdehj3tnn httã¢â‚¬â¦\n"
     ]
    }
   ],
   "source": [
    "befLowCase = df['message'][1]\n",
    "df['message'] = df['message'].apply(str.lower)\n",
    "aftLowCase = df['message'][1]\n",
    "\n",
    "print('Before was:\\n',befLowCase,'\\nNow it is:\\n',aftLowCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a5055",
   "metadata": {},
   "source": [
    "# Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4453f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58b11233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"$%&'()*+,-./:;<=>?[\\]^`{|}~\n"
     ]
    }
   ],
   "source": [
    "punctuation = punctuation.replace('@','')\n",
    "punctuation = punctuation.replace('_','')\n",
    "punctuation = punctuation.replace('#','')\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "baffdc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before was:\n",
      " rt @natgeochannel: watch #beforetheflood right here, as @leodicaprio travels the world to tackle climate change https://t.co/lkdehj3tnn httã¢â‚¬â¦ \n",
      "Now it is:\n",
      " rt @natgeochannel watch #beforetheflood right here as @leodicaprio travels the world to tackle climate change httpstcolkdehj3tnn httã¢â‚¬â¦\n"
     ]
    }
   ],
   "source": [
    "before = df['message'][1]\n",
    "df['message'] = df['message'].apply(lambda x: x.translate(str.maketrans('','',punctuation)))\n",
    "after = df['message'][1]\n",
    "\n",
    "print('Before was:\\n',before,'\\nNow it is:\\n',after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d733403",
   "metadata": {},
   "source": [
    "# Remove RT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5b9c4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text before the transformation was:\n",
      " rt @natgeochannel watch #beforetheflood right here as @leodicaprio travels the world to tackle climate change httpstcolkdehj3tnn httã¢â‚¬â¦ \n",
      "Now it is:\n",
      "  @natgeochannel watch #beforetheflood right here as @leodicaprio travels the world to tackle climate change httpstcolkdehjtnn httã¢â‚¬â¦\n"
     ]
    }
   ],
   "source": [
    "before = df['message'][1]\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'\\d+|rt', '', x))\n",
    "after = df['message'][1]\n",
    "\n",
    "print('The text before the transformation was:\\n',before,'\\nNow it is:\\n',after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ab155",
   "metadata": {},
   "source": [
    "# Remove # and @ from the message and create another columns for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "91f52822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags_mentions(text):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
    "    mentions = re.findall(r\"@(\\w+)\", text)\n",
    "    cleaned_text = re.sub(r\"#(\\w+)|@(\\w+)\", \"\", text)\n",
    "    return hashtags, mentions, cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "09e029a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'], df['mentions'], df['cleaned_message'] = zip(*df['message'].apply(extract_hashtags_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "40e4c506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tiniebeany]</td>\n",
       "      <td>climate change is an interesting hustle as it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@natgeochannel watch #beforetheflood right he...</td>\n",
       "      <td>793124211518832641</td>\n",
       "      <td>[beforetheflood]</td>\n",
       "      <td>[natgeochannel, leodicaprio]</td>\n",
       "      <td>watch  right here as  travels the world to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>fabulous leonardo #dicaprios film on #climate ...</td>\n",
       "      <td>793124402388832256</td>\n",
       "      <td>[dicaprios, climate]</td>\n",
       "      <td>[youtube]</td>\n",
       "      <td>fabulous leonardo  film on  change is brillian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@mick_fanning just watched this amazing docum...</td>\n",
       "      <td>793124635873275904</td>\n",
       "      <td>[]</td>\n",
       "      <td>[mick_fanning]</td>\n",
       "      <td>just watched this amazing documentary by leo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>@cnalive pranita biswasi a lutheran from odis...</td>\n",
       "      <td>793125156185137153</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cnalive]</td>\n",
       "      <td>pranita biswasi a lutheran from odisha gives...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1   @natgeochannel watch #beforetheflood right he...   \n",
       "2          1  fabulous leonardo #dicaprios film on #climate ...   \n",
       "3          1   @mick_fanning just watched this amazing docum...   \n",
       "4          2   @cnalive pranita biswasi a lutheran from odis...   \n",
       "\n",
       "              tweetid              hashtags                      mentions  \\\n",
       "0  792927353886371840                    []                  [tiniebeany]   \n",
       "1  793124211518832641      [beforetheflood]  [natgeochannel, leodicaprio]   \n",
       "2  793124402388832256  [dicaprios, climate]                     [youtube]   \n",
       "3  793124635873275904                    []                [mick_fanning]   \n",
       "4  793125156185137153                    []                     [cnalive]   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0   climate change is an interesting hustle as it...  \n",
       "1    watch  right here as  travels the world to t...  \n",
       "2  fabulous leonardo  film on  change is brillian...  \n",
       "3    just watched this amazing documentary by leo...  \n",
       "4    pranita biswasi a lutheran from odisha gives...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae3b59",
   "metadata": {},
   "source": [
    "# Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b600052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_message'] = df['cleaned_message'].apply(lambda x: re.sub(r'htt\\S+|www\\S+', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4493e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         climate change is an interesting hustle as it...\n",
       "1          watch  right here as  travels the world to t...\n",
       "2        fabulous leonardo  film on  change is brillian...\n",
       "3          just watched this amazing documentary by leo...\n",
       "4          pranita biswasi a lutheran from odisha gives...\n",
       "                               ...                        \n",
       "43938    dear \\nyeah right human mediated climate chang...\n",
       "43939    what will your respective paies do to prevent ...\n",
       "43940      un poll shows climate change is the lowest o...\n",
       "43941      i still canqt believe this gif of taehyung s...\n",
       "43942      \\n\\nthe wealthy  fossil fuel industry know c...\n",
       "Name: cleaned_message, Length: 41033, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89994c",
   "metadata": {},
   "source": [
    "# stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b9761256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wutyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "657bffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "51ec212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8408452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_message'] = df['cleaned_message'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d79e4e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        climate change interesting hustle global warmi...\n",
       "1          watch right travels world tackle climate change\n",
       "2        fabulous leonardo film change brilliant watch via\n",
       "3        watched amazing documentary leonardodicaprio c...\n",
       "4        pranita biswasi lutheran odisha gives testimon...\n",
       "                               ...                        \n",
       "43938    dear yeah right human mediated climate change ...\n",
       "43939     respective paies prevent climate change globally\n",
       "43940    un poll shows climate change lowest global con...\n",
       "43941    still canqt believe gif taehyung saved human r...\n",
       "43942    wealthy fossil fuel industry know climate chan...\n",
       "Name: cleaned_message, Length: 41033, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c691e",
   "metadata": {},
   "source": [
    "# Removal of emoticon and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ded96717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_emoticon(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "  \n",
    "    cleaned_text = emoji_pattern.sub(r'', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "87bf057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_message'] = df['cleaned_message'].apply(remove_emoji_emoticon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6190053",
   "metadata": {},
   "source": [
    "# Spellchecking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd394870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spello in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: nltk<4,>=3.4.5 in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (from spello) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (from nltk<4,>=3.4.5->spello) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wutyi\\anaconda3\\lib\\site-packages (from click->nltk<4,>=3.4.5->spello) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install spello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b4b07558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spello.model.SpellCorrectionModel at 0x194275d4190>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model to fix spelling after installing spello\n",
    "from spello.model import SpellCorrectionModel\n",
    "\n",
    "sp = SpellCorrectionModel(language='en')\n",
    "sp.load('C:\\\\Users\\\\wutyi\\\\anaconda3\\\\Lib\\\\site-packages\\\\spello\\\\en.pkl\\\\en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ef85ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_message'] = df['cleaned_message'].apply(lambda x: sp.spell_correct(x)['spell_corrected_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8342c5d",
   "metadata": {},
   "source": [
    "# Handling of contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "51c77782",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"ain't\": \"am not / are not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a1b2365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f8da747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_message'] = df['cleaned_message'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a85ed379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dictionary of negations to handle\n",
    "#negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                 \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                 \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                 \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                 \"mustn't\":\"must not\"}\n",
    "\n",
    "\n",
    "#neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "#def remove_contractions(text):\n",
    "    #return neg_pattern.sub(lambda x: negations_dic[x.group()], text)\n",
    "\n",
    "#df['message'] = df['message'].apply(lambda x: remove_contractions(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08901f4c",
   "metadata": {},
   "source": [
    "# Tokenization of words\n",
    "\n",
    "\n",
    "TweetTokenizer from the package nltk.tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b782f33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wutyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tiniebeany]</td>\n",
       "      <td>[climate, change, interesting, hostel, global,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@natgeochannel watch #beforetheflood right he...</td>\n",
       "      <td>793124211518832641</td>\n",
       "      <td>[beforetheflood]</td>\n",
       "      <td>[natgeochannel, leodicaprio]</td>\n",
       "      <td>[watch, right, travels, world, tackle, climate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>fabulous leonardo #dicaprios film on #climate ...</td>\n",
       "      <td>793124402388832256</td>\n",
       "      <td>[dicaprios, climate]</td>\n",
       "      <td>[youtube]</td>\n",
       "      <td>[fabulous, leonardo, film, change, brilliant, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@mick_fanning just watched this amazing docum...</td>\n",
       "      <td>793124635873275904</td>\n",
       "      <td>[]</td>\n",
       "      <td>[mick_fanning]</td>\n",
       "      <td>[watched, amazing, documentary, leonardodicapr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>@cnalive pranita biswasi a lutheran from odis...</td>\n",
       "      <td>793125156185137153</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cnalive]</td>\n",
       "      <td>[granite, bias, lutheran, odis, gives, testimo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1   @natgeochannel watch #beforetheflood right he...   \n",
       "2          1  fabulous leonardo #dicaprios film on #climate ...   \n",
       "3          1   @mick_fanning just watched this amazing docum...   \n",
       "4          2   @cnalive pranita biswasi a lutheran from odis...   \n",
       "\n",
       "              tweetid              hashtags                      mentions  \\\n",
       "0  792927353886371840                    []                  [tiniebeany]   \n",
       "1  793124211518832641      [beforetheflood]  [natgeochannel, leodicaprio]   \n",
       "2  793124402388832256  [dicaprios, climate]                     [youtube]   \n",
       "3  793124635873275904                    []                [mick_fanning]   \n",
       "4  793125156185137153                    []                     [cnalive]   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  [climate, change, interesting, hostel, global,...  \n",
       "1  [watch, right, travels, world, tackle, climate...  \n",
       "2  [fabulous, leonardo, film, change, brilliant, ...  \n",
       "3  [watched, amazing, documentary, leonardodicapr...  \n",
       "4  [granite, bias, lutheran, odis, gives, testimo...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "tok = TweetTokenizer()\n",
    "df['cleaned_message'] = df['cleaned_message'].apply(lambda x: tok.tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f581ae1",
   "metadata": {},
   "source": [
    "# Lemmatization of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6514c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wutyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "29d04e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wutyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\wutyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "05233d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         climate change hostel global planet yes suv boom\n",
      "1           watch right travel world tackle climate change\n",
      "2        fabulous leonardo film change brilliant watch via\n",
      "3        documentary leonardodicaprio climate change th...\n",
      "4        granite bias lutheran odis testimony effect cl...\n",
      "                               ...                        \n",
      "43938    dear yeah right human climate change chinese u...\n",
      "43939       respective pas prevent climate change globally\n",
      "43940       un poll show climate change low global concern\n",
      "43941    still cannot gif tachyons human race global ev...\n",
      "43942    wealthy fossil fuel industry climate change re...\n",
      "Name: cleaned_message, Length: 41033, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmat(w_list):\n",
    "    lemm_sentence = []\n",
    "    for w in w_list:\n",
    "        pos_tag = nltk.pos_tag([w])[0]\n",
    "        # Default to noun\n",
    "        wtag = wordnet.NOUN\n",
    "        \n",
    "        if pos_tag[1].startswith('J'):\n",
    "            wtag = wordnet.ADJ\n",
    "        elif pos_tag[1].startswith('N'):\n",
    "            wtag = wordnet.NOUN\n",
    "        elif pos_tag[1].startswith('R'):\n",
    "            wtag = wordnet.ADV\n",
    "        elif pos_tag[1].startswith('V'):\n",
    "            continue\n",
    "\n",
    "        lemmetized_word = lemmatizer.lemmatize(w, pos=wtag)\n",
    "        lemm_sentence.append(lemmetized_word)\n",
    "    return lemm_sentence\n",
    "\n",
    "\n",
    "df['cleaned_message'] = df['cleaned_message'].apply(lambda x: ' '.join(lemmat(x)))\n",
    "\n",
    "\n",
    "print(df['cleaned_message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bc64a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['cleaned_message'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "339ad18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_twitterdata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
